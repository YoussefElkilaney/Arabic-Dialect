{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff015c42",
   "metadata": {},
   "source": [
    "##### This is the script that's in use when a form is submit to the server with a valid POST request. When server is up, it loads all files and variables, and when POST request is recieved with text, it's sent to `mlpredict` function and return goes back to caller and is set in dictinary context for page to read and show as prediction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load recognitionService/ML_Model/mlpredict.py\n",
    "# from ArabicDialect.ArabicDialect.settings import STATIC_ROOT\n",
    "\n",
    "from cProfile import label\n",
    "from csv import Dialect\n",
    "from operator import inv\n",
    "from django.conf import settings\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn import pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "from .preprocessing import clean\n",
    "\n",
    "STATIC_URL = 'static'\n",
    "\n",
    "DialectsJsonURI = f'{STATIC_URL}/dialects.json'\n",
    "\n",
    "datasetIds = pd.read_csv(f'{STATIC_URL}/dialect_dataset.csv', usecols=['dialect'])\n",
    "\n",
    "try:\n",
    "    with open(DialectsJsonURI, 'r') as dialectsJson:\n",
    "        dialectsMap = json.load(dialectsJson)\n",
    "except Exception:\n",
    "    # This is a csv link that lists all countries with their 2 digit code\n",
    "    # Not sure if PL code belongs to Poland, but I'm gussing PL here actually means\n",
    "    # Palestine, according to `Figure 1` in `arXiv:2005.06557` linked in the pdf.\n",
    "    # 'https://pkgstore.datahub.io/core/country-list/data_csv/data/d7c9d7cfb42cb69f4422dec222dbbaa8/data_csv.csv'\n",
    "    country_iso = f'{STATIC_URL}/data_csv.csv'\n",
    "    country_iso = pd.read_csv(country_iso, index_col='Code')\n",
    "    country_iso.loc['PL'] = 'Palestine'\n",
    "\n",
    "    countries = country_iso.loc[datasetIds,'Name'].values\n",
    "    dialectsMap = dict(sorted(zip(datasetIds, countries)))\n",
    "\n",
    "    with open(DialectsJsonURI, 'w+') as dialectsJson:\n",
    "        json.dump(dialectsMap, dialectsJson)\n",
    "\n",
    "\n",
    "dialects = sorted(datasetIds.dialect.unique())\n",
    "label_map = dict(zip(dialects, range(len(dialects))))\n",
    "inv_label_map = dict(enumerate(dialects))\n",
    "\n",
    "# this model from pkl, already contains the pipeline for\n",
    "# transforming inputs then predicting\n",
    "sgd = joblib.load(f'{STATIC_URL}/SGDClassifier_0.pkl')\n",
    "\n",
    "eclf = joblib.load(f'{STATIC_URL}/VotingClassifier.pkl')\n",
    "\n",
    "def mlpredict(text):\n",
    "    input = clean(text)\n",
    "    pred = sgd.predict([input])[0]\n",
    "    # pred = eclf.predict([input])[0]\n",
    "    pred = inv_label_map[pred]\n",
    "    dialect = dialectsMap.get(pred, f'{pred} -- dialect not found')\n",
    "    return dialect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e8c27",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60de727",
   "metadata": {},
   "source": [
    "##### The same idea goes here aswell, model is initiated and prepared when server is up, and on POST request, text is fed into `dlpredict` function and then it returns either top 1 or top k results, as the pipeline is set to rerturn [line 19]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load recognitionService/DL_Model/dlpredict.py\n",
    "from transformers import *\n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "from recognitionService.ML_Model.mlpredict import dialectsMap\n",
    "\n",
    "STATIC_URL = 'static'\n",
    "\n",
    "# A dummy tiny model for testing ~ 30mb\n",
    "dlModel = f'{STATIC_URL}/bert-tiny-mnli'\n",
    "\n",
    "# AraBERT trained on QADI Arabic Dialect dataset - 1 epoch ~ 600mb\n",
    "# dlModel = f'{STATIC_URL}/ArabiceDialect_BERT'\n",
    "\n",
    "pipe = pipeline('sentiment-analysis', model=dlModel,\n",
    "                max_length=90, truncation=True,\n",
    "                # uncomment this line to return all classes with probs\n",
    "                # return_all_scores=True, \n",
    "                )\n",
    "\n",
    "def dlpredict(text):\n",
    "    s = pipe([text])\n",
    "\n",
    "    # incase return all class with probs\n",
    "    # topK = 3\n",
    "    # preds = sorted([(x['score'], x['label']) for x in s[0]], reverse=True)\n",
    "    # pred = preds[:topK]\n",
    "    # dialect = [(dialectsMap.get(x,f'{x[1]} -- dialect not found'), x[0]) for x in pred]\n",
    "\n",
    "    pred = s[0]['label']\n",
    "    dialect = dialectsMap.get(pred, f'{pred} -- dialect not found')\n",
    "    return dialect\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
