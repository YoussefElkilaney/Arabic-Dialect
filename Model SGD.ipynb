{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f895a0",
   "metadata": {},
   "source": [
    "# Modelling & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef85166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be3cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('cleanData.csv', index_col=0)\n",
    "except Exception:\n",
    "    %run -i preprocess.py\n",
    "    data = pd.read_csv('cleanData.csv', index_col=0)\n",
    "\n",
    "datasetIds = pd.read_csv('dialect_dataset.csv', index_col='id')\n",
    "dialects = sorted(datasetIds.dialect.unique())\n",
    "label_map = dict(zip(dialects, range(len(dialects))))\n",
    "inv_label_map = dict(enumerate(dialects))\n",
    "\n",
    "# Figure 7 in `arxiv:2005.06557`\n",
    "ordered_dialects = 'IQ YE OM BH KW SA AE QA DZ MA LY TN EG SD JO PL LB SY'.split()\n",
    "ordered_dialects = [label_map[x] for x in ordered_dialects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e23e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_x = [inv_label_map[x] for x in data.dialect.value_counts().index]\n",
    "bins_y = data.dialect.value_counts() # / len(data)None\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(bins_x, bins_y)\n",
    "plt.title('Dialects counts sorted')\n",
    "plt.tight_layout()\n",
    "\n",
    "# this sorting matches the order in paper, it's here for illustration\n",
    "freq = data.dialect.value_counts()\n",
    "# transform the returned series ordered_dialects with index as key\n",
    "freq = dict(zip(freq.index, freq))\n",
    "# sort the keys in freq by ordered dialects\n",
    "freq = [freq[x] for x in ordered_dialects_ids] # / len(data)None\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(ordered_dialects, freq)\n",
    "plt.title('Dialects counts as ordered in paper')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ded2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
    "                                     GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             balanced_accuracy_score, classification_report)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "limit = 5000\n",
    "mini = data.groupby('dialect').sample(limit)\n",
    "feature = mini.text\n",
    "target = mini.dialect\n",
    "\n",
    "# # splitting the data into target and feature\n",
    "# # getting out of memmory everytime -> sol: Out Of Core (partial_fit)\n",
    "# # Out Of Core + Stratified KFold, maybe coming soon\n",
    "# feature = data.text\n",
    "# target = data.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature, target, \n",
    "                                                    test_size =.1, random_state=42)\n",
    "\n",
    "# # splitting into train and validation\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "#                                                   test_size =.1, random_state=42)\n",
    "\n",
    "def fit_model(pipeline, parameters, random_search, itr=0, interactive=1, X_train=X_train,\n",
    "              y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "    if interactive:\n",
    "        print(f'Performing {random_search.__class__.__name__} ...')\n",
    "        print('pipeline:', [name for name, _ in pipeline.steps])\n",
    "        print('parameters:')\n",
    "        pprint(parameters)\n",
    "    t0 = time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print('done in %0.3fs' % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    if interactive:\n",
    "        print('Best score: %0.3f' % random_search.best_score_)\n",
    "        print('Best parameters set:')\n",
    "        best_parameters = random_search.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "        # make prediction and print accuracy\n",
    "        prediction = random_search.predict(X_test)\n",
    "        print(f'Accuracy score is {balanced_accuracy_score(y_test, prediction):.2f}')\n",
    "\n",
    "        print(classification_report(y_test, prediction, labels=ordered_dialects_ids))\n",
    "    \n",
    "    joblib.dump(random_search.best_estimator_, \n",
    "                f'{pipeline.named_steps.clf.__class__.__name__}_{itr}.pkl',\n",
    "                compress = 1)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92e3a6",
   "metadata": {},
   "source": [
    "## SGDClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_fit_sgd(random_state=42, n_iter=1, itr=0, interactive=1):\n",
    "    # make pipeline\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', SGDClassifier()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # make param grid\n",
    "    parameters = {\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'vect__max_features': (None, 5000, 10000, 50000),\n",
    "        'vect__ngram_range': ((1, 2), (1, 3), (1, 4), (1, 5), (1, 6)),\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),\n",
    "        'clf__max_iter': (20, 50, 80, 100, 150),\n",
    "        'clf__alpha': (0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01,),\n",
    "        'clf__penalty': ('l2', 'elasticnet'),\n",
    "    }\n",
    "\n",
    "    # create and fit the model\n",
    "    random_search_sgd = RandomizedSearchCV(pipeline, parameters, cv=5, verbose=1,\n",
    "                                           scoring='f1_macro', n_iter=n_iter, n_jobs=-1,\n",
    "                                           random_state=random_state)\n",
    "\n",
    "    fit_model(pipeline, parameters, random_search_sgd, itr=itr, interactive=interactive)\n",
    "    \n",
    "    return random_search_sgd\n",
    "\n",
    "random_search_sgd = random_search_fit_sgd(interactive=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc33209",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = random_search_sgd.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_test, preds, labels=ordered_dialects_ids))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(14,10))\n",
    "ConfusionMatrixDisplay.from_estimator(random_search_sgd.best_estimator_, X_test,\n",
    "                                      y_test, ax=ax, labels=ordered_dialects_ids,\n",
    "                                      normalize='true', values_format='.2f', );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2157a5",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "Quite bad, so dropped it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_fit_rf(random_state=42, n_iter=1, itr=0, interactive=1):\n",
    "    # make pipeline\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', RandomForestClassifier()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # make param grid\n",
    "    parameters = {\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'vect__max_features': (None, 5000, 10000, 50000),\n",
    "        'vect__ngram_range': ((1, 2), (1, 3), (1, 4), (1, 5), (1, 6)),\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),\n",
    "        'clf__criterion': ('gini', 'entropy'),\n",
    "        'clf__max_depth': list(range(10,110,10,),)+[None,],\n",
    "        'clf__max_features': ['auto', 'sqrt'],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "    }\n",
    "\n",
    "    # create and fit the model\n",
    "    random_search_rf = RandomizedSearchCV(pipeline, parameters, cv=5, verbose=1,\n",
    "                                           scoring='f1_macro', n_iter=1, n_jobs=-1,\n",
    "                                           random_state=42)\n",
    "\n",
    "    fit_model(pipeline, parameters, random_search_rf, itr=itr, interactive=interactive)\n",
    "    \n",
    "    return random_search_rf\n",
    "\n",
    "random_search_rf = random_search_fit_rf(interactive=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = random_search_rf.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_test, preds, labels=ordered_dialects_ids))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(14,10))\n",
    "ConfusionMatrixDisplay.from_estimator(random_search_rf.best_estimator_, X_test, y_test,\n",
    "                                      ax=ax, labels=ordered_dialects_ids, normalize='true',\n",
    "                                      values_format='.2f', );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e714c9",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "Train 5 models on 5 folds, with each fold represent a sample of 5k sample from every dialect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b22725",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5\n",
    "SGDs = []\n",
    "for i in range(num):\n",
    "    print('-'*10, f'{i+1} / {num}', '-'*10)\n",
    "    sgd = random_search_fit_sgd(itr=i, interactive=0)\n",
    "    SGDs.append(sgd.best_estimator_)\n",
    "\n",
    "SGDLbls = [f'SGDClassifier_{x}' for x in range(1,6)]\n",
    "for clf, label in zip(SGDs, SGDLbls):\n",
    "    scores = cross_val_score(clf, X_test, y_test, error_score='raise',\n",
    "                             scoring='f1_macro', cv=5)\n",
    "    print(f'F1 score: {scores.mean():0.2f} (+/- {scores.std():0.2f}) [{label}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa31080",
   "metadata": {},
   "source": [
    "#### I've already trained/fit my models, and I don't want to retrain so, I'm listing the objects of my trained models as a list then fitting labelEncoder of the Ensembler, then save the ensembler as pkl, and it'll automatically write all the trained models, and later when trying to use it for inference, load and use.\n",
    "#### Ensembler expects predictions from base estimatros to be int and we have encoded classes into numbers, now it'll call predict on all classes for given inputs and return a result of majority answer for every input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=SGDs, voting='hard')\n",
    "\n",
    "eclf.estimators_ = SGDs\n",
    "eclf.le_ = LabelEncoder().fit(y_train)\n",
    "eclf.classes_ = eclf.le_.classes_\n",
    "\n",
    "# This will likely be a big file, that contains 5 classes, each is ~ 120mb\n",
    "print(joblib.dump(eclf, f'{eclf.__class__.__name__}.pkl', compress = 1))\n",
    "eclf = joblib.load('VotingClassifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d15c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
