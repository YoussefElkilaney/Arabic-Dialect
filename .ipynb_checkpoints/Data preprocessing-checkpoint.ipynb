{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfa4ca8",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4c05b",
   "metadata": {},
   "source": [
    "After fetching the data, preprocessing is done on each sample via `apply` method on dataframe rows.\n",
    "\n",
    "Preprocessing pipeline include the following steps:\n",
    "- removing emojis\n",
    "- removing mentions and links\n",
    "- removing diacritics\n",
    "- removing punctuations\n",
    "- normalizing similar arabic characters to one unicode\n",
    "- replacing numbers, repeating_char, newlines, and multiple whitespaces with one whitespace, with a final strip for spaces on sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66879303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "try:\n",
    "    rawData = pd.read_json('arabicDialects.json', encoding='utf-8')\n",
    "except Exception:\n",
    "    %run -i fetch.py\n",
    "    rawData = pd.read_json('arabicDialects.json', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8567c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "try:\n",
    "    rawData = pd.read_json('arabicDialects.json', encoding='utf-8',\n",
    "                           typ='series', convert_axes=False)\n",
    "except Exception as e:\n",
    "    %run -i fetch.py\n",
    "    rawData = pd.read_json('arabicDialects.json', encoding='utf-8',\n",
    "                           typ='series', convert_axes=False)\n",
    "\n",
    "rawData.index = rawData.index.astype('int64')\n",
    "\n",
    "datasetIds = pd.read_csv('dialect_dataset.csv', index_col='id')\n",
    "datasetIds = datasetIds.loc[rawData.index]\n",
    "\n",
    "# https://github.com/motazsaad/process-arabic-text/blob/f950f9b3aeba13ac3c16cd10502a8eafbdfa1262/clean_arabic_text.py\n",
    "# I found this useful and quick to use, I hope it doesn't violate the task guideline.\n",
    "# I'm using it and I know how it works and I'm referring to the links for fidelity.\n",
    "\n",
    "# I modified methods that remove unwanted characters by replacing with empty string\n",
    "# to replace with a space, and then a final step to remove redudant white spaces.\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "punctuations_map = {p:' ' for p in punctuations_list}\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    arabic_diacritics = re.compile('''\n",
    "                 ّ    | # Tashdid\n",
    "                 َ    | # Fatha\n",
    "                 ً    | # Tanwin Fath\n",
    "                 ُ    | # Damma\n",
    "                 ٌ    | # Tanwin Damm\n",
    "                 ِ    | # Kasra\n",
    "                 ٍ    | # Tanwin Kasr\n",
    "                 ْ    | # Sukun\n",
    "                 ـ     # Tatwil/Kashida\n",
    "             ''', re.VERBOSE)\n",
    "    return re.sub(arabic_diacritics, '', text)\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub('[إأآا]', 'ا', text)\n",
    "    text = re.sub('ى', 'ي', text)\n",
    "    text = re.sub('ؤ', 'ء', text)\n",
    "    text = re.sub('ئ', 'ء', text)\n",
    "    text = re.sub('ة', 'ه', text)\n",
    "    text = re.sub('گ', 'ك', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "#     translator = str.maketrans('', '', punctuations_list)\n",
    "    translator = str.maketrans(punctuations_map)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "#     # I think skipping هههه might make sense\n",
    "#     # this actually have some meaning/sentiment\n",
    "#     text = re.sub(r'([^ه])\\1+', r'\\1', text)\n",
    "#     # but then remove redundant هه more than, say 5\n",
    "#     # this is a common limit.\n",
    "#     # the pattern might be distorted, but it counts\n",
    "#     # correctly {4,}\\1\n",
    "#     return re.sub(r'(ه)\\1{4,}', r'\\1\\1\\1\\1', text)\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "\n",
    "def remove_mentions_links(text):\n",
    "    # https://stackoverflow.com/a/56659272/12896502\n",
    "    mention_links_pattern = re.compile(r'(@|https?)\\S+|#')\n",
    "    return mention_links_pattern.sub(r' ', text)\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # https://stackoverflow.com/a/33417311/12896502\n",
    "#     emojis_pattern = re.compile('['\n",
    "#             u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "#             u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "#             u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "#             u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "#                                ']+', flags=re.UNICODE)\n",
    "    # more emoji, previous wasn't enough, hopefully not overkill\n",
    "    # https://stackoverflow.com/a/58356570/12896502\n",
    "    emoj = re.compile('['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002500-\\U00002BEF'  # chinese char\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        u'\\U0001f926-\\U0001f937'\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u'\\u2640-\\u2642' \n",
    "        u'\\u2600-\\u2B55'\n",
    "        u'\\u200d'\n",
    "        u'\\u23cf'\n",
    "        u'\\u23e9'\n",
    "        u'\\u231a'\n",
    "        u'\\ufe0f'  # dingbats\n",
    "        u'\\u3030'\n",
    "                      ']+', re.UNICODE)\n",
    "    return emoj.sub(' ', text)\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r'[0-9]+', ' ', text)\n",
    "\n",
    "#     https://stackoverflow.com/a/4134156/12896502\n",
    "    return re.sub(r'[\\u0660-\\u0669]+', ' ', text)\n",
    "\n",
    "\n",
    "def remove_newlines(text):\n",
    "    return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "def cleaning_text(text):\n",
    "    # Regarding removing URLs and mentions, according\n",
    "    # to the paper I'm following for developing the model,\n",
    "    # they normalized, rather than removed them, by\n",
    "    # replacing them by URL and USER tokens, respectively.\n",
    "    # So, this may be another trial to go for.\n",
    "    # Paper; arxiv:2005.06557\n",
    "    pipeline = [\n",
    "        remove_emojis,\n",
    "        remove_mentions_links,\n",
    "        remove_diacritics,\n",
    "        normalize_arabic,\n",
    "        remove_punctuations,\n",
    "        remove_numbers,\n",
    "        remove_repeating_char,\n",
    "        remove_newlines,\n",
    "        remove_whitespaces,\n",
    "    ]\n",
    "    \n",
    "    clean_text = text\n",
    "    for step in pipeline:\n",
    "        clean_text = step(clean_text)\n",
    "    return clean_text.strip()\n",
    "\n",
    "datasetIds['text'] = rawData.apply(cleaning_text)\n",
    "\n",
    "dialects = sorted(datasetIds.dialect.unique())\n",
    "label_map = dict(zip(dialects, range(len(dialects))))\n",
    "inv_label_map = dict(enumerate(dialects))\n",
    "\n",
    "datasetIds['dialect'] = datasetIds.dialect.apply(lambda x:label_map[x])\n",
    "\n",
    "data = datasetIds[datasetIds['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576afa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save cleaned data\n",
    "data.to_csv('cleanData.csv')\n",
    "\n",
    "# to read it later\n",
    "pd.read_csv('cleanData.csv', index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
